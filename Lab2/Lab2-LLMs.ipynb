{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b10d85",
   "metadata": {},
   "source": [
    "# TODO\n",
    "rendere disponibile mha per vedere i pesi dell'attenzione così da capire cosa sfrutta per predire il token i+1-esimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378f0ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71220 8005\n",
      "(13534, 360)\n",
      "LA D\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "\n",
    "class DanteSet(Dataset):\n",
    "    def __init__(self, src_path, train=True):\n",
    "        with open(src_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.t_p = 0.9\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        if train:\n",
    "            self.ids = self.enc.encode_ordinary(self.data[:int(len(self.data)*self.t_p)])\n",
    "        else:\n",
    "            self.ids = self.enc.encode_ordinary(self.data[int(len(self.data)*self.t_p):])\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids[idx], self.ids[idx+1]\n",
    "    \n",
    "train_ds = DanteSet(\"inferno.txt\", train=True)\n",
    "test_ds = DanteSet(\"inferno.txt\", train=False)\n",
    "print(len(train_ds), len(test_ds))\n",
    "print(train_ds[0])\n",
    "print(train_ds.enc.decode(train_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5516019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " maladetta, fredda e \n",
      "\n",
      "lar mi trassi.\n",
      "\n",
      "Ed \n",
      "\n",
      " si rivolse per la str \n",
      "\n",
      "\n",
      "  tal che convien che l \n",
      "\n",
      "adetta, fredda e gre \n",
      "\n",
      " mi trassi.\n",
      "\n",
      "Ed ecc \n",
      "\n",
      " rivolse per la strada \n",
      "\n",
      "  tal che convien che lui \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fake dataloader\n",
    "import random\n",
    "import torch\n",
    "seq_length = 8\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "def make_batch(batch_size, seq_length, shuffle=True, targets=True):\n",
    "    batch = torch.zeros(batch_size, seq_length, dtype=torch.long)\n",
    "    if targets:\n",
    "        target_batch = torch.zeros(batch_size, seq_length, dtype=torch.long)\n",
    "    else:\n",
    "        target_batch = None\n",
    "    # print(batch.shape)\n",
    "    for b in range(batch_size):\n",
    "        starting_index = b\n",
    "        if shuffle:\n",
    "            starting_index = random.randint(0, len(train_ds)-seq_length)\n",
    "        for seq in range(seq_length):\n",
    "            sequence = train_ds[starting_index+seq]\n",
    "            batch[b][seq] = sequence[0]\n",
    "            if targets:\n",
    "                target_batch[b][seq] = sequence[1]\n",
    "    return batch, target_batch\n",
    "batch, targets = make_batch(batch_size, seq_length, shuffle, targets=True)\n",
    "for b in batch:\n",
    "    print(train_ds.enc.decode(b.tolist()), \"\\n\")\n",
    "\n",
    "for b in targets:\n",
    "    print(train_ds.enc.decode(b.tolist()), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b55e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a27672",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 16\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3a63f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GPTLLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self._vocab_size = config.vocab_size\n",
    "        self._n_embd = config.n_embd\n",
    "        self._block_size = config.block_size\n",
    "        self.token_embedding_table = nn.Embedding(self._vocab_size, self._n_embd)\n",
    "        self.pos_embedding_table = nn.Embedding(self._block_size, self._n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(4)])\n",
    "        self.ln_f = LayerNorm(self._n_embd, bias=config.bias)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self, x, targets = None):\n",
    "        (B, T) = x.shape\n",
    "        tok_emb = self.token_embedding_table(x) # (B, T, C)\n",
    "        pos_emb = self.pos_embedding_table(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -self._block_size:]\n",
    "                logits, _ = self.forward(idx_cond)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat((idx, idx_next), dim=-1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTLLM(config).to(\"cuda\")\n",
    "#print(gpt)\n",
    "x, y = make_batch(4, config.block_size, shuffle)\n",
    "x = x.to(\"cuda\")\n",
    "y = y.to(\"cuda\")\n",
    "print(x.shape, y.shape)\n",
    "logits, loss = gpt(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7642317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the model for 10000 iterations...\n",
      "training on  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 508/10000 [00:08<02:36, 60.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! me assue;\n",
      "  m'io mis, in quest sta\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1012/10000 [00:16<02:30, 59.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! rirmeunara che sc,\n",
      "\n",
      "  qual ma cantilleam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1509/10000 [00:23<02:22, 59.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "e non vedzer mondo cambo manile reve sciolte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2006/10000 [00:31<02:16, 58.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "  mutre se converier l'orel fosti par anbra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2510/10000 [00:39<02:05, 59.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>,\n",
      "  come lascienza gente ogia`, e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3010/10000 [00:48<02:06, 55.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!ilranando\n",
      " , che da la voi tra puffa proved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3512/10000 [00:56<01:49, 59.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>.\n",
      "\n",
      "Inbero dimmi ci subsciceOr com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4007/10000 [01:04<01:49, 54.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>,\n",
      "  quel viso miseri fuzzo possondomito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4507/10000 [01:13<01:40, 54.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "  come cos` la` d'un ruina la palizn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5009/10000 [01:21<01:31, 54.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>.\n",
      "\n",
      "\n",
      "\n",
      "Noi che quevi le le fummo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5513/10000 [01:29<01:15, 59.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "La mentra coda braccorse nel Vor de la sp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6010/10000 [01:37<01:07, 59.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>.\n",
      "\n",
      "Inferno: << taprai disse, poi quest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6508/10000 [01:45<00:57, 60.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "E a si` Era e 'ce\n",
      "\n",
      "Al cerco,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7012/10000 [01:53<00:50, 59.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>.\n",
      "\n",
      "E se' quella fidan e` la venuta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7509/10000 [02:01<00:41, 59.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>.\n",
      "\n",
      "Elli a quel se miei ghine pre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8006/10000 [02:09<00:34, 58.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "El 'l Casito e sod acc da Corn paesi.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8510/10000 [02:17<00:25, 58.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "\n",
      "Si` di tratto\n",
      "  a qua levero campo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9007/10000 [02:25<00:16, 59.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "  credi da poi a seppi, Fiorenhiam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 9511/10000 [02:33<00:08, 58.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "\n",
      "E morta z\n",
      " ,' com' monto de la\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:41<00:00, 61.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>.\n",
      "\n",
      "S' io avaro l'altissiun\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "iterations = int(1e4)\n",
    "print(f\"training the model for {iterations} iterations...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gpt = GPTLLM(config).to(device)\n",
    "print(f\"training on \", device)\n",
    "optim = torch.optim.AdamW(gpt.parameters(), lr=6e-4)\n",
    "for i in tqdm(range(iterations)):\n",
    "    x, y = make_batch(4, config.block_size, shuffle)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optim.zero_grad()\n",
    "    logits, loss = gpt(x, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if (i + 1) % 500 == 0 : \n",
    "        print(train_ds.enc.decode(gpt.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=config.block_size).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15624968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!>>.\n",
      "\n",
      "Noi: <<Maestro; posdiiglio,, che' siega, fin,\n",
      "  pori confitto dauglia, di scorpiese e per lo sacinto.\n",
      "\n",
      "E io cor sia pensire in forza mia\n",
      "\n",
      "di proceduglia avante.\n",
      "\n",
      "<<Dolta drannia che pone pote>>.\n",
      "\n",
      "Poi miaan lor inver'arte levar tenere sanza.\n",
      "\n",
      "E per che dentrocciate il tristo, dostizator\n",
      "  e\n",
      "\n",
      "Non si torcenda comincaper \"Tutti.\n",
      "\n",
      "\n",
      "I' di quel suon mal sospesi.\n",
      "\n",
      "Traentro, e\n",
      "  portta ebbi, suo mi si` aspazzi tutti>>,\n",
      "  e io; e disio,\n",
      "  maestro: <<Gia ch'io nomi e Senuoi,\n",
      "  o iri' son le parole conaro, vogne a mezza;\n",
      "  sen giu` tristizia` d'unque;\n",
      "\n",
      "  una si le sueelli piu` lasciando!\n",
      "\n",
      "I' che si squanza lungo\n",
      "\n",
      "deli e dor diere li e li oreci ch'i'armi fredi,\n",
      "  gridai con 'l duca mio mio diss si governammo e chi tutta,\n",
      "  guarda re, e` che solo:\n",
      "\n",
      "\"Non sanza s'io andare, e non sospetti.\n",
      "\n",
      "Mospesi e` mai\n",
      "\n",
      "Traova,\n",
      "\n",
      "Ne' i pietiine,\n",
      "  per appiens, che viss'io ti vittoria\n",
      "  'nosollo e` fermo com' 'l mondo era amore>>,\n",
      "  gridando l'amine sospdi per sbig;\n",
      "\n",
      "  disse 'l d lodmo per lo mondoore,\n",
      "  per loendo torno calto dir padre aria coion rimiri\n",
      "  guardo facevan saran lauda manza e nel tosto.\n",
      "\n",
      "Intaldo.\n",
      "\n",
      "Io che ' tu tema fui, bello\n",
      "  intesi ne' non al correndo d'una\n",
      "\n",
      "  perchiosi si rabbtre al varo che se' passi\n",
      "  Io le cosce difetti sesti pre levar,\n",
      "\n",
      "se il reo avuango.\n",
      "\n",
      "<<Lai son levQuamlli a recare di sangue,\n",
      "  e dalcun p specil fondo, pria porto mio;\n",
      "  dalberga se'io suff a puoiugnoso labbia!\n",
      "\n",
      "<<ming'io Sc anima senno li'appasta\n",
      "\n",
      "  li orec fermo ov'ud nasesi! e sia morti!>>,\n",
      "\n",
      "unsi\n",
      "Parti 'l duca disse\n",
      "\n",
      "  disse de leon lasciammo inferno a no le labbra.\n",
      "\n",
      "Noi che lor some!\n",
      "\n",
      "Inferno a noilo Tace a mentro ver'io suffi!>>.\n",
      "\n",
      "I' lascorse fondo,\n",
      "\n",
      "Pata gia` lo mondo\n",
      "  si dimoro a la pena elli a lor chi; ma dimosta si volerpiuol piu investro;\n",
      "  mi corbi, gravi la facotta.\n",
      "\n",
      "\n",
      "Qui, che d'altro ci torte, il puote,\n",
      " \n",
      "  Acho il subaggio ad tutta sia de liile eriri.\n",
      "\n",
      "Aacazzi 'l buonhi froine,\n",
      "\n",
      "Don pero`: <<Diti, tuo` gia;\n",
      "  per far le guardar aver fres menato affare rub\n",
      "  dicendo: <<Tutti, di, e non non poiia\n",
      "\n",
      " iche li reci vilta lor s'ho costretti,\n",
      "  e con sia assai e 'l grandi\n",
      "  e 'ntelle star accondino,\n",
      "\n",
      " Spi tra ' puote ritabbia,\n",
      "  elli eravamo in gini>>.\n",
      "\n",
      "Tal la mia quand'se non vi rispuose,\n",
      "  risposta gi` giva intace,\n",
      "  ch'el ver' la penutto che rimida.\n",
      "\n",
      "<<FigS' era questi converire e`: <<O anima?>>,\n",
      "Tizzos 'ntpper fuor di distorse d'\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(train_ds.enc.decode(gpt.generate(context, max_new_tokens = 1024)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siccome transformers non mi funziona con il notebook, il codice di questa parte si trova il Lab2-LLMs\n"
     ]
    }
   ],
   "source": [
    "print(\"Siccome transformers non mi funziona con il notebook, il codice di questa parte si trova il Lab2-LLMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy)\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4492796",
   "metadata": {},
   "source": [
    "si può scaricare un modello open source per fare zero shot contest question answering (non usare chatgpt3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
