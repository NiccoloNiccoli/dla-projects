{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=128):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "global steps_done\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = F.softmax(policy_net(state), dim=-1)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "\n",
    "\n",
    "def boltzmann_select_action(state, policy, temperature):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    # Compute the exponentiated Q-values\n",
    "    exp_q_values = torch.exp(policy(state) / temperature)\n",
    "    # Compute the probabilities using the Boltzmann distribution\n",
    "    probabilities = exp_q_values / torch.sum(exp_q_values)\n",
    "    # Compute the log probabilities\n",
    "    log_probs = torch.log(probabilities)\n",
    "    # Sample an action from the probabilities\n",
    "    action = torch.multinomial(probabilities, 1).item()\n",
    "    return action, log_probs[action]\n",
    "\n",
    "def boltzmann_select_action_mod(state, policy_net, temperature):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    q_values = policy_net(state)\n",
    "    q_values_detached = q_values.detach()\n",
    "\n",
    "    # Subtract the max value for numerical stability\n",
    "    q_values_detached -= torch.max(q_values_detached)\n",
    "\n",
    "    # Compute softmax probabilities\n",
    "    exp_q_values = torch.exp(q_values_detached / temperature)\n",
    "    probabilities = exp_q_values / torch.sum(exp_q_values)\n",
    "\n",
    "    # Check for any NaN or negative values and replace them with a small positive number\n",
    "    if torch.isnan(probabilities).any() or (probabilities < 0).any():\n",
    "        probabilities[torch.isnan(probabilities) | (probabilities < 0)] = 1e-10\n",
    "\n",
    "    # Normalize the probabilities again in case they don't sum to 1\n",
    "    probabilities /= probabilities.sum()\n",
    "\n",
    "    action = torch.multinomial(probabilities, 1).item()\n",
    "    log_prob = torch.log(probabilities[action])\n",
    "    return action, log_prob\n",
    "\n",
    "def select_action_eps(state, policy_net):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = F.softmax(policy_net(state), dim=-1)\n",
    "    if sample > eps_threshold:\n",
    "        action = probs.argmax()\n",
    "    else:\n",
    "        action = torch.multinomial(probs, num_samples=1)\n",
    "    return action.item(), torch.log(probs[action])\n",
    "\n",
    "def update_policy(rewards, log_probs, optimizer_policy, std_baseline_values = None, optimizer_value = None):\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for r in rewards[::-1]:\n",
    "        discounted_reward = r + 0.99 * discounted_reward\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9) # normalize\n",
    "\n",
    "\n",
    "    policy_loss = []\n",
    "    if std_baseline_values is not None:\n",
    "        for log_prob, R, baseline in zip(log_probs, returns, std_baseline_values):\n",
    "            policy_loss.append(-log_prob * (R - baseline))\n",
    "    else:\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "            \n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    optimizer_policy.step()\n",
    "\n",
    "    if optimizer_value is not None:\n",
    "        value_loss = F.mse_loss(std_baseline_values.squeeze(1), returns)\n",
    "        optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        optimizer_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "env_render = gym.make(env_name, render_mode = \"human\")\n",
    "\n",
    "policy_net = NeuralNet(env.observation_space.shape[0], env.action_space.n)\n",
    "value_net = NeuralNet(env.observation_space.shape[0], 1)\n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer_value = optim.Adam(value_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward: -222.02066719434805 - Avg Reward: -222.02066719434805\n",
      "Episode 150 - Reward: -301.14937200604425 - Avg Reward: -176.56855014278923\n",
      "Episode 300 - Reward: -115.2943792516303 - Avg Reward: -185.26372916682013\n",
      "Episode 450 - Reward: -157.33550104088562 - Avg Reward: -168.07469438430218\n",
      "Episode 600 - Reward: -226.7300520382715 - Avg Reward: -181.5900048880132\n",
      "Episode 750 - Reward: -115.55054287417413 - Avg Reward: -186.34216698657525\n",
      "Episode 900 - Reward: -201.97017312441767 - Avg Reward: -172.08914726320936\n",
      "Episode 1050 - Reward: -103.57071141742203 - Avg Reward: -169.51934945903542\n",
      "Episode 1200 - Reward: -75.66753591981183 - Avg Reward: -191.32307124125077\n",
      "Episode 1350 - Reward: -133.1714607652639 - Avg Reward: -185.55744786947304\n",
      "Episode 1500 - Reward: -135.1597003762604 - Avg Reward: -193.79809167527517\n",
      "Episode 1650 - Reward: -340.7800349807271 - Avg Reward: -185.064790250999\n",
      "Episode 1800 - Reward: -7.121707333457195 - Avg Reward: -194.0847782543694\n",
      "Episode 1950 - Reward: -272.4948960906545 - Avg Reward: -197.65695089905415\n",
      "Episode 2100 - Reward: -278.2577760085482 - Avg Reward: -212.52504141687436\n",
      "Episode 2250 - Reward: -136.71588915521048 - Avg Reward: -212.58529093707355\n",
      "Episode 2400 - Reward: -227.96067612645 - Avg Reward: -200.112748906691\n",
      "Episode 2550 - Reward: -141.51381206510598 - Avg Reward: -196.96406533895745\n",
      "Episode 2700 - Reward: -204.5843653059344 - Avg Reward: -197.07031169288345\n",
      "Episode 2850 - Reward: -192.59198314413956 - Avg Reward: -214.7671728595739\n",
      "Episode 3000 - Reward: -210.79781513289032 - Avg Reward: -189.2861412749056\n",
      "Episode 3150 - Reward: -124.59021523570554 - Avg Reward: -178.42274894828714\n",
      "Episode 3300 - Reward: -130.43115566711887 - Avg Reward: -182.32240242959347\n",
      "Episode 3450 - Reward: -185.2258100674102 - Avg Reward: -168.88909686118362\n",
      "Episode 3600 - Reward: -178.28597703482345 - Avg Reward: -165.93086074938904\n",
      "Episode 3750 - Reward: -91.33511672665709 - Avg Reward: -156.70690328102495\n",
      "Episode 3900 - Reward: -126.67841672899749 - Avg Reward: -139.9680733364707\n",
      "Episode 4050 - Reward: -237.21952035470215 - Avg Reward: -137.06013537030395\n",
      "Episode 4200 - Reward: -159.1170027696408 - Avg Reward: -142.00216712400285\n",
      "Episode 4350 - Reward: -120.2405066692663 - Avg Reward: -129.45761415548307\n",
      "Episode 4500 - Reward: -145.69043496814112 - Avg Reward: -125.25954193810942\n",
      "Episode 4650 - Reward: -102.17112707829371 - Avg Reward: -124.45709457806437\n",
      "Episode 4800 - Reward: -79.57920736830586 - Avg Reward: -132.91297711052727\n",
      "Episode 4950 - Reward: -130.8462428594172 - Avg Reward: -133.5752837878839\n",
      "Episode 5100 - Reward: -127.89458152789862 - Avg Reward: -129.54929305576297\n",
      "Episode 5250 - Reward: -189.08520024442564 - Avg Reward: -132.11662130502464\n",
      "Episode 5400 - Reward: -106.03912183921966 - Avg Reward: -124.30563079309276\n",
      "Episode 5550 - Reward: -124.97273866898522 - Avg Reward: -135.73902529008438\n",
      "Episode 5700 - Reward: -113.81872842379241 - Avg Reward: -129.87423758795305\n",
      "Episode 5850 - Reward: -109.25472121470241 - Avg Reward: -126.19402496110511\n",
      "Episode 6000 - Reward: -122.27372303318313 - Avg Reward: -131.13374146369335\n",
      "Episode 6150 - Reward: -139.5845599857267 - Avg Reward: -132.65714687293584\n",
      "Episode 6300 - Reward: -138.2988959147778 - Avg Reward: -128.37470366408087\n",
      "Episode 6450 - Reward: -171.16048838934788 - Avg Reward: -133.1080632316652\n",
      "Episode 6600 - Reward: -157.3138586223376 - Avg Reward: -128.26177607182834\n",
      "Episode 6750 - Reward: -108.92630945986696 - Avg Reward: -125.95672381998719\n",
      "Episode 6900 - Reward: -99.88097544788937 - Avg Reward: -127.89018862442816\n",
      "Episode 7050 - Reward: -92.07848153590383 - Avg Reward: -124.46331603218239\n",
      "Episode 7200 - Reward: -181.81847377396647 - Avg Reward: -133.06317331210042\n",
      "Episode 7350 - Reward: -129.8715133574994 - Avg Reward: -127.47037410930791\n",
      "Episode 7500 - Reward: -104.22129732713066 - Avg Reward: -129.5786167157846\n",
      "Episode 7650 - Reward: -151.85127866780917 - Avg Reward: -130.9100280360282\n",
      "Episode 7800 - Reward: -131.0278026070603 - Avg Reward: -129.8413894335964\n",
      "Episode 7950 - Reward: -139.12797532010583 - Avg Reward: -124.73932288911332\n",
      "Episode 8100 - Reward: -134.48922755349918 - Avg Reward: -125.67036424020252\n",
      "Episode 8250 - Reward: -164.10186980587667 - Avg Reward: -130.98609059554252\n",
      "Episode 8400 - Reward: -147.64220804027073 - Avg Reward: -132.97183259605717\n",
      "Episode 8550 - Reward: -126.32342777298143 - Avg Reward: -131.80945317566238\n",
      "Episode 8700 - Reward: -97.94995395541012 - Avg Reward: -125.94897642373488\n",
      "Episode 8850 - Reward: -113.2505104981766 - Avg Reward: -127.29685508131702\n",
      "Episode 9000 - Reward: -125.66732515958446 - Avg Reward: -126.78878166371146\n",
      "Episode 9150 - Reward: -121.07233379253935 - Avg Reward: -125.15190914932934\n",
      "Episode 9300 - Reward: -117.65752373770303 - Avg Reward: -133.80943785756847\n",
      "Episode 9450 - Reward: -164.91593941447258 - Avg Reward: -130.45556348256665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m states \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#action, log_prob = select_action(state, policy_net)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# action, log_prob = boltzmann_select_action(state, policy_net, temperature)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# action, log_prob = select_action_eps(state, policy_net)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mboltzmann_select_action_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     next_state, reward, term, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     20\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Cell \u001b[1;32mIn[61], line 23\u001b[0m, in \u001b[0;36mboltzmann_select_action_mod\u001b[1;34m(state, policy_net, temperature)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboltzmann_select_action_mod\u001b[39m(state, policy_net, temperature):\n\u001b[0;32m     22\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 23\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     q_values_detached \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Subtract the max value for numerical stability\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\dla\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\dla\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\dla\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\dla\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\envs\\dla\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_baseline = True\n",
    "\n",
    "num_episodes = 10000\n",
    "avg_rewards = deque(maxlen=150)\n",
    "values = None\n",
    "temperature = 1.0\n",
    "global epsilon\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    states = []\n",
    "    for t in count():\n",
    "        #action, log_prob = select_action(state, policy_net)\n",
    "        # action, log_prob = boltzmann_select_action(state, policy_net, temperature)\n",
    "        # action, log_prob = select_action_eps(state, policy_net)\n",
    "        action, log_prob = boltzmann_select_action_mod(state, policy_net, temperature)\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        states.append(state)\n",
    "        state = next_state\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            break\n",
    "    if use_baseline:\n",
    "        values = value_net(torch.tensor(states, dtype=torch.float32))\n",
    "        values = (values - values.mean()) / (values.std() + 1e-9) # normalize\n",
    "    update_policy(rewards, log_probs, optimizer_policy, values, optimizer_value)\n",
    "    avg_rewards.append(sum(rewards))\n",
    "    \n",
    "    if episode % 150 == 0:\n",
    "        print(f\"Episode {episode} - Reward: {sum(rewards)} - Avg Reward: {np.mean(avg_rewards)}\")\n",
    "    temperature *= 0.999\n",
    "    # if episode % 1500 == 0:\n",
    "       #  temperature -= 0.05\n",
    "    \n",
    "    #if np.mean(avg_rewards) > 200 and episode > 100:\n",
    "    #    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f\"policy_{env_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 259.68489652746996\n",
      "Average Reward: 176.7315248638158\n"
     ]
    }
   ],
   "source": [
    "env_render = gym.make(env_name, render_mode = \"human\")\n",
    "env_render = gym.make(env_name)\n",
    "deq = deque(maxlen=100)\n",
    "for m in range(100):\n",
    "    state, info = env_render.reset()\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        action, _ = select_action(state, policy_net)\n",
    "        state, reward, term, trunc, _ = env_render.step(action)\n",
    "        total_reward += reward\n",
    "        if term or trunc:\n",
    "            break\n",
    "    deq.append(total_reward)\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Average Reward: {np.mean(deq)}\")\n",
    "env_render.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
