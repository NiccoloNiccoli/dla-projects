{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=128):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = F.softmax(policy_net(state), dim=-1)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "\n",
    "\n",
    "def update_policy(rewards, log_probs, optimizer_policy, std_baseline_values = None, optimizer_value = None):\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for r in rewards[::-1]:\n",
    "        discounted_reward = r + 0.99 * discounted_reward\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9) # normalize\n",
    "\n",
    "\n",
    "    policy_loss = []\n",
    "    if std_baseline_values is not None:\n",
    "        for log_prob, R, baseline in zip(log_probs, returns, std_baseline_values):\n",
    "            policy_loss.append(-log_prob * (R - baseline))\n",
    "    else:\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "            \n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    optimizer_policy.step()\n",
    "\n",
    "    if optimizer_value is not None:\n",
    "        value_loss = F.mse_loss(std_baseline_values.squeeze(1), returns)\n",
    "        optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        optimizer_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "env_render = gym.make(env_name, render_mode = \"human\")\n",
    "\n",
    "policy_net = NeuralNet(env.observation_space.shape[0], env.action_space.n)\n",
    "value_net = NeuralNet(env.observation_space.shape[0], 1)\n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer_value = optim.Adam(value_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward: -387.79145183747715 - Avg Reward: -387.79145183747715\n",
      "Episode 100 - Reward: -371.44402140854476 - Avg Reward: -283.4727905960478\n",
      "Episode 200 - Reward: -64.3440700754955 - Avg Reward: -179.36925727585017\n",
      "Episode 300 - Reward: -93.180336179497 - Avg Reward: -108.54195807691464\n",
      "Episode 400 - Reward: 3.542618977622027 - Avg Reward: -108.06957258292965\n",
      "Episode 500 - Reward: -189.53711814849322 - Avg Reward: -83.63851261783132\n",
      "Episode 600 - Reward: -40.27523320665416 - Avg Reward: -30.18043016992075\n",
      "Episode 700 - Reward: -15.61083559310822 - Avg Reward: -13.942305555519397\n",
      "Episode 800 - Reward: 16.69398692111811 - Avg Reward: -8.416047814670804\n",
      "Episode 900 - Reward: -93.27271479894037 - Avg Reward: 7.3520389952558105\n",
      "Episode 1000 - Reward: -45.49195198574582 - Avg Reward: 32.14753319610036\n",
      "Episode 1100 - Reward: -18.093579720998136 - Avg Reward: 23.075630397722406\n",
      "Episode 1200 - Reward: -5.188329038563694 - Avg Reward: 28.084911462885184\n",
      "Episode 1300 - Reward: 4.354825902135929 - Avg Reward: 31.458201745471353\n",
      "Episode 1400 - Reward: 252.3877315457568 - Avg Reward: 42.28345609970499\n",
      "Episode 1500 - Reward: 6.200502605320864 - Avg Reward: 3.98901066490809\n",
      "Episode 1600 - Reward: 40.6670028226194 - Avg Reward: 34.199173174631674\n",
      "Episode 1700 - Reward: -8.177862943438015 - Avg Reward: 42.27348045057974\n",
      "Episode 1800 - Reward: 19.83465759369912 - Avg Reward: 42.2632553183813\n",
      "Episode 1900 - Reward: 48.04338580667422 - Avg Reward: 43.797890575193996\n",
      "Episode 2000 - Reward: 30.68294667699351 - Avg Reward: 48.55603635230581\n",
      "Episode 2100 - Reward: 0.7938912480578324 - Avg Reward: 45.868104317956885\n",
      "Episode 2200 - Reward: -25.372170942061874 - Avg Reward: 10.508566960740803\n",
      "Episode 2300 - Reward: -12.37590001448524 - Avg Reward: -1.861164407294616\n",
      "Episode 2400 - Reward: 19.351705775567396 - Avg Reward: 27.041422082851533\n",
      "Episode 2500 - Reward: 188.81046986543265 - Avg Reward: 40.11162089004827\n",
      "Episode 2600 - Reward: -18.48552193748425 - Avg Reward: 61.36733793058459\n",
      "Episode 2700 - Reward: 10.406908339527263 - Avg Reward: 49.259742590378444\n",
      "Episode 2800 - Reward: 34.47646073298765 - Avg Reward: 27.995432608938437\n",
      "Episode 2900 - Reward: 49.71822084534952 - Avg Reward: 43.664813605715715\n",
      "Episode 3000 - Reward: 17.647438679539448 - Avg Reward: 78.65725523157602\n",
      "Episode 3100 - Reward: 114.52736753349124 - Avg Reward: 93.30588125392154\n",
      "Episode 3200 - Reward: -3.359175978312109 - Avg Reward: 0.5810739019833217\n",
      "Episode 3300 - Reward: -45.689103108265314 - Avg Reward: 20.290739208457623\n",
      "Episode 3400 - Reward: 5.037396361121864 - Avg Reward: 80.16764106288285\n",
      "Episode 3500 - Reward: 36.448487861101654 - Avg Reward: 36.73325891046505\n",
      "Episode 3600 - Reward: 6.305559541833048 - Avg Reward: 68.75091119453054\n",
      "Episode 3700 - Reward: 138.60452756976176 - Avg Reward: 96.3354402444099\n",
      "Episode 3800 - Reward: 137.46997005307668 - Avg Reward: 60.11856892759525\n",
      "Episode 3900 - Reward: -21.09379783163581 - Avg Reward: 102.3895544954846\n",
      "Episode 4000 - Reward: 47.2224859384045 - Avg Reward: 69.9946607007766\n",
      "Episode 4100 - Reward: -2.589779189780373 - Avg Reward: 106.92150972191828\n",
      "Episode 4200 - Reward: 10.423886940760553 - Avg Reward: 43.72526615136843\n",
      "Episode 4300 - Reward: 21.979135432730388 - Avg Reward: -25.025450782800977\n",
      "Episode 4400 - Reward: 13.923320565723515 - Avg Reward: -38.39123620426874\n",
      "Episode 4500 - Reward: 13.581823091465012 - Avg Reward: -6.8739291806172025\n",
      "Episode 4600 - Reward: 6.873444910416211 - Avg Reward: 100.16732148116782\n",
      "Episode 4700 - Reward: 38.35054170878314 - Avg Reward: -8.784745993437284\n",
      "Episode 4800 - Reward: 141.6382836143594 - Avg Reward: 38.071370493303846\n",
      "Episode 4900 - Reward: 1.811698041965613 - Avg Reward: 75.47996197814491\n",
      "Episode 5000 - Reward: 260.1704503952461 - Avg Reward: 117.36588234840929\n",
      "Episode 5100 - Reward: -9.131457734662845 - Avg Reward: 138.41532566931892\n",
      "Episode 5200 - Reward: -9.042520536016326 - Avg Reward: 132.07882670080514\n",
      "Episode 5300 - Reward: 205.11674183976038 - Avg Reward: 95.17606593696864\n",
      "Episode 5400 - Reward: 119.54995182082715 - Avg Reward: 138.16780040592369\n",
      "Episode 5500 - Reward: 155.74986499513614 - Avg Reward: 131.02906613520844\n",
      "Episode 5600 - Reward: 236.73711292108055 - Avg Reward: 118.39360723785758\n",
      "Episode 5700 - Reward: 164.63286635967154 - Avg Reward: 118.7004439398245\n",
      "Episode 5800 - Reward: 232.36109716161116 - Avg Reward: 91.30715621523228\n",
      "Episode 5900 - Reward: 278.14459094497875 - Avg Reward: 118.85252751453501\n",
      "Episode 6000 - Reward: 137.82810221523468 - Avg Reward: 93.98915856025478\n",
      "Episode 6100 - Reward: 1.0477176234305006 - Avg Reward: 123.92941221852634\n",
      "Episode 6200 - Reward: 236.4068120728725 - Avg Reward: 124.47167523828693\n",
      "Episode 6300 - Reward: 1.7011346247836343 - Avg Reward: 107.91167982110377\n",
      "Episode 6400 - Reward: 27.63267630121537 - Avg Reward: 112.91633830683985\n",
      "Episode 6500 - Reward: -23.028917549868623 - Avg Reward: 82.83723074835021\n",
      "Episode 6600 - Reward: 142.0925858113144 - Avg Reward: 111.2152322114624\n",
      "Episode 6700 - Reward: 1.2244660520099728 - Avg Reward: 127.89305313858138\n",
      "Episode 6800 - Reward: 33.35078788093412 - Avg Reward: 94.91984409373379\n",
      "Episode 6900 - Reward: 230.6990630831519 - Avg Reward: 124.53945271693911\n",
      "Episode 7000 - Reward: 133.1580241707511 - Avg Reward: 116.26378040613972\n",
      "Episode 7100 - Reward: 224.6796401643052 - Avg Reward: 131.15808850288738\n",
      "Episode 7200 - Reward: 162.5734099700266 - Avg Reward: 127.87265035011457\n",
      "Episode 7300 - Reward: 172.7740689572913 - Avg Reward: 143.59953229772486\n",
      "Episode 7400 - Reward: 49.90943376082496 - Avg Reward: 157.50075962006574\n",
      "Episode 7500 - Reward: -0.9570497282454511 - Avg Reward: 130.62988791181448\n",
      "Episode 7600 - Reward: 258.2316336978125 - Avg Reward: 117.23874321859091\n",
      "Episode 7700 - Reward: 267.1127641420425 - Avg Reward: 139.67977800812082\n",
      "Episode 7800 - Reward: 13.780759185389485 - Avg Reward: 150.380168445428\n",
      "Episode 7900 - Reward: 189.65453939434963 - Avg Reward: 149.90550403652702\n",
      "Episode 8000 - Reward: 205.6382656106845 - Avg Reward: 113.7364079666437\n",
      "Episode 8100 - Reward: 256.95658127748686 - Avg Reward: 116.23832381140701\n",
      "Episode 8200 - Reward: 129.33377016450115 - Avg Reward: 82.94459656754945\n",
      "Episode 8300 - Reward: 173.93043983703313 - Avg Reward: 131.5398598366066\n",
      "Episode 8400 - Reward: 151.3359416605936 - Avg Reward: 123.44723963101298\n",
      "Episode 8500 - Reward: -0.6985236663296348 - Avg Reward: 142.55892395265934\n",
      "Episode 8600 - Reward: -2.4933195994014596 - Avg Reward: 156.5504295267753\n",
      "Episode 8700 - Reward: 282.0018774598509 - Avg Reward: 144.81336390963781\n",
      "Episode 8800 - Reward: 238.11200299012563 - Avg Reward: 143.49482751449943\n",
      "Episode 8900 - Reward: -13.460926548605045 - Avg Reward: 136.76710924010976\n",
      "Episode 9000 - Reward: 138.63397210283648 - Avg Reward: 129.1775598950827\n",
      "Episode 9100 - Reward: -4.963826707924355 - Avg Reward: 104.19425827796634\n",
      "Episode 9200 - Reward: -6.885042596512832 - Avg Reward: 105.74024548768669\n",
      "Episode 9300 - Reward: 129.5817632051335 - Avg Reward: 80.1451536721475\n",
      "Episode 9400 - Reward: 197.25163745663394 - Avg Reward: 110.22769836580893\n",
      "Episode 9500 - Reward: 248.9293820439156 - Avg Reward: 143.063270896756\n",
      "Episode 9600 - Reward: 15.852618177821157 - Avg Reward: 93.96971008453055\n",
      "Episode 9700 - Reward: 51.850691632634806 - Avg Reward: 90.151216454025\n",
      "Episode 9800 - Reward: 241.70638361890838 - Avg Reward: 80.2122989797681\n",
      "Episode 9900 - Reward: -47.80266244737554 - Avg Reward: 91.75336840706673\n"
     ]
    }
   ],
   "source": [
    "use_baseline = True\n",
    "\n",
    "num_episodes = 10000\n",
    "avg_rewards = deque(maxlen=100)\n",
    "values = None\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    states = []\n",
    "    for t in count():\n",
    "        action, log_prob = select_action(state, policy_net)\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        states.append(state)\n",
    "        state = next_state\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            break\n",
    "    if use_baseline:\n",
    "        values = value_net(torch.tensor(states, dtype=torch.float32))\n",
    "        values = (values - values.mean()) / (values.std() + 1e-9) # normalize\n",
    "    update_policy(rewards, log_probs, optimizer_policy, values, optimizer_value)\n",
    "    avg_rewards.append(sum(rewards))\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} - Reward: {sum(rewards)} - Avg Reward: {np.mean(avg_rewards)}\")\n",
    "    \n",
    "    if np.mean(avg_rewards) > 200 and episode > 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f\"policy_{env_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 42.50689201443953\n",
      "Average Reward: 34.538132552292616\n"
     ]
    }
   ],
   "source": [
    "env_render = gym.make(env_name, render_mode = \"human\")\n",
    "env_render = gym.make(env_name)\n",
    "deq = deque(maxlen=100)\n",
    "for m in range(100):\n",
    "    state, info = env_render.reset()\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        action, _ = select_action(state, policy_net)\n",
    "        state, reward, term, trunc, _ = env_render.step(action)\n",
    "        total_reward += reward\n",
    "        if term or trunc:\n",
    "            break\n",
    "    deq.append(total_reward)\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Average Reward: {np.mean(deq)}\")\n",
    "env_render.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
