{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=128):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = F.softmax(policy_net(state), dim=-1)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "\n",
    "\n",
    "def update_policy(rewards, log_probs, optimizer_policy, std_baseline_values = None, optimizer_value = None):\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for r in rewards[::-1]:\n",
    "        discounted_reward = r + 0.99 * discounted_reward\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9) # normalize\n",
    "\n",
    "\n",
    "    policy_loss = []\n",
    "    if std_baseline_values is not None:\n",
    "        for log_prob, R, baseline in zip(log_probs, returns, std_baseline_values):\n",
    "            policy_loss.append(-log_prob * (R - baseline))\n",
    "    else:\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "            \n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    optimizer_policy.step()\n",
    "\n",
    "    if optimizer_value is not None:\n",
    "        value_loss = F.mse_loss(std_baseline_values.squeeze(1), returns)\n",
    "        optimizer_value.zero_grad()\n",
    "        value_loss.backward()\n",
    "        optimizer_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "env_render = gym.make(env_name, render_mode = \"human\")\n",
    "\n",
    "policy_net = NeuralNet(env.observation_space.shape[0], env.action_space.n)\n",
    "value_net = NeuralNet(env.observation_space.shape[0], 1)\n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "optimizer_value = optim.Adam(value_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward: -387.79145183747715 - Avg Reward: -387.79145183747715\n",
      "Episode 100 - Reward: -371.44402140854476 - Avg Reward: -283.4727905960478\n",
      "Episode 200 - Reward: -64.3440700754955 - Avg Reward: -179.36925727585017\n",
      "Episode 300 - Reward: -93.180336179497 - Avg Reward: -108.54195807691464\n",
      "Episode 400 - Reward: 3.542618977622027 - Avg Reward: -108.06957258292965\n",
      "Episode 500 - Reward: -189.53711814849322 - Avg Reward: -83.63851261783132\n",
      "Episode 600 - Reward: -40.27523320665416 - Avg Reward: -30.18043016992075\n",
      "Episode 700 - Reward: -15.61083559310822 - Avg Reward: -13.942305555519397\n",
      "Episode 800 - Reward: 16.69398692111811 - Avg Reward: -8.416047814670804\n",
      "Episode 900 - Reward: -93.27271479894037 - Avg Reward: 7.3520389952558105\n",
      "Episode 1000 - Reward: -45.49195198574582 - Avg Reward: 32.14753319610036\n",
      "Episode 1100 - Reward: -18.093579720998136 - Avg Reward: 23.075630397722406\n",
      "Episode 1200 - Reward: -5.188329038563694 - Avg Reward: 28.084911462885184\n",
      "Episode 1300 - Reward: 4.354825902135929 - Avg Reward: 31.458201745471353\n",
      "Episode 1400 - Reward: 252.3877315457568 - Avg Reward: 42.28345609970499\n",
      "Episode 1500 - Reward: 6.200502605320864 - Avg Reward: 3.98901066490809\n",
      "Episode 1600 - Reward: 40.6670028226194 - Avg Reward: 34.199173174631674\n",
      "Episode 1700 - Reward: -8.177862943438015 - Avg Reward: 42.27348045057974\n",
      "Episode 1800 - Reward: 19.83465759369912 - Avg Reward: 42.2632553183813\n",
      "Episode 1900 - Reward: 48.04338580667422 - Avg Reward: 43.797890575193996\n",
      "Episode 2000 - Reward: 30.68294667699351 - Avg Reward: 48.55603635230581\n",
      "Episode 2100 - Reward: 0.7938912480578324 - Avg Reward: 45.868104317956885\n",
      "Episode 2200 - Reward: -25.372170942061874 - Avg Reward: 10.508566960740803\n",
      "Episode 2300 - Reward: -12.37590001448524 - Avg Reward: -1.861164407294616\n",
      "Episode 2400 - Reward: 19.351705775567396 - Avg Reward: 27.041422082851533\n",
      "Episode 2500 - Reward: 188.81046986543265 - Avg Reward: 40.11162089004827\n",
      "Episode 2600 - Reward: -18.48552193748425 - Avg Reward: 61.36733793058459\n",
      "Episode 2700 - Reward: 10.406908339527263 - Avg Reward: 49.259742590378444\n",
      "Episode 2800 - Reward: 34.47646073298765 - Avg Reward: 27.995432608938437\n",
      "Episode 2900 - Reward: 49.71822084534952 - Avg Reward: 43.664813605715715\n",
      "Episode 3000 - Reward: 17.647438679539448 - Avg Reward: 78.65725523157602\n",
      "Episode 3100 - Reward: 114.52736753349124 - Avg Reward: 93.30588125392154\n",
      "Episode 3200 - Reward: -3.359175978312109 - Avg Reward: 0.5810739019833217\n",
      "Episode 3300 - Reward: -45.689103108265314 - Avg Reward: 20.290739208457623\n",
      "Episode 3400 - Reward: 5.037396361121864 - Avg Reward: 80.16764106288285\n",
      "Episode 3500 - Reward: 36.448487861101654 - Avg Reward: 36.73325891046505\n",
      "Episode 3600 - Reward: 6.305559541833048 - Avg Reward: 68.75091119453054\n",
      "Episode 3700 - Reward: 138.60452756976176 - Avg Reward: 96.3354402444099\n",
      "Episode 3800 - Reward: 137.46997005307668 - Avg Reward: 60.11856892759525\n",
      "Episode 3900 - Reward: -21.09379783163581 - Avg Reward: 102.3895544954846\n"
     ]
    }
   ],
   "source": [
    "use_baseline = True\n",
    "\n",
    "num_episodes = 10000\n",
    "avg_rewards = deque(maxlen=100)\n",
    "values = None\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    states = []\n",
    "    for t in count():\n",
    "        action, log_prob = select_action(state, policy_net)\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        states.append(state)\n",
    "        state = next_state\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            break\n",
    "    if use_baseline:\n",
    "        values = value_net(torch.tensor(states, dtype=torch.float32))\n",
    "        values = (values - values.mean()) / (values.std() + 1e-9) # normalize\n",
    "    update_policy(rewards, log_probs, optimizer_policy, values, optimizer_value)\n",
    "    avg_rewards.append(sum(rewards))\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode} - Reward: {sum(rewards)} - Avg Reward: {np.mean(avg_rewards)}\")\n",
    "    \n",
    "    if np.mean(avg_rewards) > 200 and episode > 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), f\"policy_{env_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -60.50008546373553\n",
      "Average Reward: -6.499187376281375\n"
     ]
    }
   ],
   "source": [
    "env_render = gym.make(env_name, render_mode = \"human\")\n",
    "env_render = gym.make(env_name)\n",
    "deq = deque(maxlen=100)\n",
    "for m in range(100):\n",
    "    state, info = env_render.reset()\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        action, _ = select_action(state, policy_net)\n",
    "        state, reward, term, trunc, _ = env_render.step(action)\n",
    "        total_reward += reward\n",
    "        if term or trunc:\n",
    "            break\n",
    "    deq.append(total_reward)\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Average Reward: {np.mean(deq)}\")\n",
    "env_render.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
